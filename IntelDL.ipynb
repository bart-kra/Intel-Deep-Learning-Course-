{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntelDL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNOLnjtYPr73/u0GClsUzH8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neurons as logic gates\n"
      ],
      "metadata": {
        "id": "4VUCfdNi_-1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "DfFDUceCASz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sigmoid function**\n"
      ],
      "metadata": {
        "id": "QernPL4bAHqh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h9eACtmQIO3"
      },
      "outputs": [],
      "source": [
        "#Define the sigmoid function\n",
        "def sigmoid(x):\n",
        "  \"\"\"Sigmoid function\"\"\"\n",
        "  return 1.0 / (1.0 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the sigmoid function\n",
        "vals = np.linspace(-10, 10, num=100, dtype=np.float32)\n",
        "activation = sigmoid(vals)\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "plt.plot(vals, activation)\n",
        "plt.grid(True, which='both')\n",
        "plt.axhline(y=0, color='k')\n",
        "plt.axvline(x=0, color='k')\n",
        "plt.yticks()\n",
        "plt.ylim([-0.5, 1.5]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "MyiueyO9AmPG",
        "outputId": "e504b66e-f7b3-4a6e-9180-19f71d44f6fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4708a546003b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Plot the sigmoid function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sigmoid as OR, AND, NOR and NAND gate**\n"
      ],
      "metadata": {
        "id": "5ydIKvmKCQmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to choose weights and bias to gate works properly. OR - if both inputs are 0 -> sig(z) = 0, if any of input is 1 -> sig(z) = 1"
      ],
      "metadata": {
        "id": "SV2FEvDQCcXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logic_gate(w1, w2, b):\n",
        "  #helper to create logic gate functions\n",
        "  #plug in values for weights and bias\n",
        "  return lambda x1, x2: sigmoid(w1 * x1 + w2 * x2 + b)\n",
        "\n",
        "def test_gate(gate):\n",
        "  #helper function to test the weight function\n",
        "  for a,b in (0,0), (0,1), (1,0), (1,1):\n",
        "    print(\"{}, {}: {}\".format(a, b, np.round(gate(a, b))))"
      ],
      "metadata": {
        "id": "PW1mUGEwAuDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "or_gate = logic_gate(20, 20, -10)\n",
        "test_gate(or_gate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRNCrlOgDOuv",
        "outputId": "9648114c-17ef-4ebf-9ce1-f7a926542ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0, 0: 0.0\n",
            "0, 1: 1.0\n",
            "1, 0: 1.0\n",
            "1, 1: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AND - if both or one of inputs is 0 -> sig(z) = 0, if both of them are 1 -> sig(z) = 1"
      ],
      "metadata": {
        "id": "6s0w0yAGD472"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "and_gate = logic_gate(15, 15, -20)\n",
        "test_gate(and_gate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODgcPsJqDzIo",
        "outputId": "42071017-40d5-4179-95d4-62dc0b6d3bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0, 0: 0.0\n",
            "0, 1: 0.0\n",
            "1, 0: 0.0\n",
            "1, 1: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOR - any of inputs cannot be 1 to return sig(z) = 1"
      ],
      "metadata": {
        "id": "Gc9g1JLUEZTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nor_gate = logic_gate(-20, -20, 10)\n",
        "test_gate(nor_gate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP-pHaHCEUAG",
        "outputId": "b5bb2ffd-5d93-4acb-fcca-89f6ca6e8ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0, 0: 1.0\n",
            "0, 1: 0.0\n",
            "1, 0: 0.0\n",
            "1, 1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAND - only if both inputs are 1 sig(z) = 0"
      ],
      "metadata": {
        "id": "1Q1GFZF1E6UI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nand_gate = logic_gate(-20, -20, 30)\n",
        "test_gate(nand_gate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LadY9ja0Etse",
        "outputId": "62fcb3b4-57e4-430f-e1ad-41a44c566831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0, 0: 1.0\n",
            "0, 1: 1.0\n",
            "1, 0: 1.0\n",
            "1, 1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XOR gate is impossible to perform using single neuron so we are made to use something more complex:"
      ],
      "metadata": {
        "id": "0STmdiSwF7cV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xor_gate(a, b):\n",
        "  c = or_gate(a, b)\n",
        "  d = nand_gate(a, b)\n",
        "  return and_gate(c, d)\n",
        "test_gate(xor_gate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS9VZgy-FVn2",
        "outputId": "e17f5a12-7a70-47ca-fe68-4167ebf71d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0, 0: 0.0\n",
            "0, 1: 1.0\n",
            "1, 0: 1.0\n",
            "1, 1: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**"
      ],
      "metadata": {
        "id": "jQOAojPVJXv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provided below are the following:\n",
        "\n",
        "Three weight matrices W_1, W_2 and W_3 representing the weights in each layer. The convention for these matrices is that each  Wi,j  gives the weight from neuron  i  in the previous (left) layer to neuron  j  in the next (right) layer.\n",
        "A vector x_in representing a single input and a matrix x_mat_in representing 7 different inputs.\n",
        "Two functions: soft_max_vec and soft_max_mat which apply the soft_max function to a single vector, and row-wise to a matrix.\n",
        "The goals for this exercise are:\n",
        "\n",
        "For input x_in calculate the inputs and outputs to each layer (assuming sigmoid activations for the middle two layers and soft_max output for the final layer.\n",
        "Write a function that does the entire neural network calculation for a single input\n",
        "Write a function that does the entire neural network calculation for a matrix of inputs, where each row is a single input.\n",
        "Test your functions on x_in and x_mat_in."
      ],
      "metadata": {
        "id": "rS6jGcG5LBx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_1 = np.array([[2,-1,1,4],[-1,2,-3,1],[3,-2,-1,5]])\n",
        "W_1"
      ],
      "metadata": {
        "id": "w6SohmefGn3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_2 = np.array([[3,1,-2,1],[-2,4,1,-4],[-1,-3,2,-5],[3,1,1,1]])\n",
        "W_3 = np.array([[-1,3,-2],[1,-1,-3],[3,-2,2],[1,2,1]])"
      ],
      "metadata": {
        "id": "r6h7iQp-DMZf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}